{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria2Query+ - Extension of Criteria2Query\n",
    "## Background\n",
    "Criteria2Query (C2Q) is a tool that extracts eligibility criteria from ClinicalTrials.gov. It uses CoreNLP from Stanford to extract and identify candidate entities for concept mapping to OMOP CDM Standard Vocabularies (such as LOINC and SNOMED-CT). After extraction, the entities are mapped to concepts using a Lucene-based tool, Usagi. The best candidate concept is then assigned a concept set in OHDSI's ATLAS tool and all possible candidates are provided to the user. The user can then either select the most appropriate concept set, or allow C2Q to automatically generate a JSON formatted query for ATLAS. After selection, the query is submitted to ATLAS so that the user now has a cohort for querying a variety of databases.\n",
    "\n",
    "## Motivation\n",
    "Though Usagi has some successes, its downside is a reliance on string-based similarity for scoring mappings. For example, when sent to ConceptHub, the text \"_neurological disease_\" can map to \"_neurological disorder_\" and \"_urologic disease_,\" but \"_urologic disease_\" has a score of 0.90, due to the limited character variance, so Usagi will provide \"_urologic disease_\" as its best option for mapping.\n",
    "\n",
    "## Methodology\n",
    "As an alternative first step, I propose using MetaMap as it generally performs better than Usagi at mapping concepts. Criteria2Query+ is a tool that could be used as an add-on to C2Q. C2Q+ takes the parsed eligibility criteria from C2Q and then map those terms to MetaMap using a series of options that are relatively synonymous to those of CoreNLP and OMOP CDM tools. After obtaining the CUI from the UMLS associated with OMOP CDM Standard Vocabularies, C2Q+ then maps the CUIs back to the respective Standard Vocabulary. C2Q+ then compares the MetaMap and Usagi mappings, as well as their scores, for a given term and takes the better of the two to return to the user for a mapping as the \"best option.\" It will still provide all possible concept sets for manual selection. \n",
    "\n",
    "## Evaluation\n",
    "For my dataset, I relied on the 18 clinical trials that were chosen for evaluation by Yuan et al. 2017. After the entities were extracted from the NLP step and marked with their respective domains, I compared the term in question with the mapping provided by ConceptHub and MetaMap, which I classified as correct, partially-correct, incorrect, or N/A for entities that were not mapped. For my analysis, I calculated both loose and strict recall, precision, and F-1 scores under the following definitions:\n",
    "* **Loose includes partially-correct mappings**\n",
    "* **Strict excludes partially-correct mappings**\n",
    "* $Recall = \\frac{correct \\hspace{1mm} mappings}{total \\hspace{1mm} existing \\hspace{1mm} mappings}$\n",
    "* $Precision = \\frac{correct \\hspace{1mm} mappings}{possible \\hspace{1mm} correct \\hspace{1mm} mappings}$\n",
    "* $F_1 \\hspace{1mm} Score = 2 * (\\frac{precision \\hspace{1mm} * \\hspace{1mm} recall}{precision \\hspace{1mm} + \\hspace{1mm} recall})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of NCTIDs to review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#Uncomment the below code to create a list of NCTIDs with user input\n",
    "#ids = input(\"Enter NCTIDs separated by spaces: \")\n",
    "#idsList = ids.split()\n",
    "#print(idsList)\n",
    "\n",
    "#Read csv file\n",
    "with open('trials.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    trials = list(reader)\n",
    "\n",
    "#Eliminate exterior list\n",
    "t = []\n",
    "for nct in trials:\n",
    "    t.append(nct[0])\n",
    "\n",
    "trials = t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Selenium to fetch each trial, parse the criteria, and download JSON file\n",
    "Rename the files of the form: [trial-ID].json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "#Not very reliable for len(trials) > 4\n",
    "#To attempt full automation, replace for i in range(x, y) with for t in trials and trials[i] with t\n",
    "#Current form is for small batches, which is better considering that the scrapping is not entirely reliable and\n",
    "#is not guaranteed to work 100% of the time.\n",
    "\n",
    "for i in range(0, 4):\n",
    "\n",
    "    #Connect to C2Q\n",
    "    driver = webdriver.Chrome(\"/Users/sal/Downloads/chromedriver\")\n",
    "    driver.get(\"http://www.ohdsi.org/web/criteria2query/\")\n",
    "\n",
    "    #Load clinical trial\n",
    "    nctid = driver.find_element_by_id('nctid')\n",
    "    fetchct = driver.find_element_by_id('fetchct')\n",
    "    nctid.send_keys(trials[i])\n",
    "    ActionChains(driver).click(fetchct).perform()\n",
    "\n",
    "    #Parse the criteria\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "    parse = wait.until(EC.element_to_be_clickable((By.ID, 'start')))\n",
    "    ActionChains(driver).click(parse).perform()\n",
    "\n",
    "    #Extract JSON text\n",
    "    download = wait.until(EC.element_to_be_clickable((By.ID, 'downloadfile')))\n",
    "    ActionChains(driver).click(download).perform()\n",
    "\n",
    "    time.sleep(5)\n",
    "    driver.quit()\n",
    "\n",
    "    #Move the file from downloads to the JSON_Formatted_Trials folder\n",
    "    source = '/Users/sal/Downloads/Criteria2Query.json'\n",
    "    destination = '/Users/sal/Desktop/DBMI/SymbolicMethods/ConceptMapping/JSON_Formatted_Trials'\n",
    "\n",
    "    shutil.move(source, destination)\n",
    "\n",
    "    #Rename the file in the JSON folder\n",
    "    if os.path.exists('Criteria2Query.json'):\n",
    "        src = os.path.realpath('Criteria2Query.json')\n",
    "        os.rename('Criteria2Query.json', trials[i] +'.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run extraction of on each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "#Reformat the terms from the .json file\n",
    "#extraction[] contains all of the trials in the format {text : [{term : domain}]}\n",
    "extraction = []\n",
    "\n",
    "for nct in trials:\n",
    "    with open(\"./JSON_Formatted_Trials/\"+ nct + \".json\", \"r\") as read_file:\n",
    "        data = json.load(read_file)\n",
    "\n",
    "    trial = pd.DataFrame.from_dict(data)    \n",
    "    exclusion = trial.iat[0,0]\n",
    "    inclusion = trial.iat[1,0]\n",
    "\n",
    "    #trial[0,0] is the exclusion criteria\n",
    "    #trial[1,0] is the inclusion criteria\n",
    "    \n",
    "    m = []\n",
    "    for e in exclusion:\n",
    "        phrase = e.get('sents')[0].get('text')\n",
    "        q = []\n",
    "        for t in e.get('sents')[0].get('terms'):\n",
    "            entity = t.get('text')\n",
    "            domain = t.get('categorey')\n",
    "            q.append([entity, domain])\n",
    "        m.append({phrase:q})\n",
    "\n",
    "    for i in inclusion:\n",
    "        phrase = i.get('sents')[0].get('text')\n",
    "        q = []\n",
    "        for t in i.get('sents')[0].get('terms'):\n",
    "            entity = t.get('text')\n",
    "            domain = t.get('categorey')\n",
    "            q.append([entity, domain])\n",
    "        m.append({phrase:q})\n",
    "    \n",
    "    extraction.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open MetaMap servers and send terms to MetaMap in a txt file\n",
    "Include line breaks and end each file with '\\n'\n",
    "\n",
    "Export the output of each to (NCTID)\\_MM.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Starts the SKR/Medpost Part-of-Speech Tagger and Word Sense Disambiguation Servers\n",
    "#The WSD server takes about a minute to fully load, so the TimeoutExpired exception is just used to move onto the next\n",
    "import subprocess\n",
    "\n",
    "mm_path = \"/Users/sal/Desktop/DBMI/SymbolicMethods/ConceptMapping/public_mm\"\n",
    "subprocess.run([mm_path + \"/bin/skrmedpostctl start\"], shell = True, capture_output=True)\n",
    "subprocess.run([mm_path + \"/bin/wsdserverctl start\"], shell = True, capture_output=True, timeout = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Running MetaMap\n",
    "run_mm = mm_path + \"/bin/metamap18 -AIGy+ --negex -R CVX,HCPCS,ICD10PCS,LNC,RXNORM,SNOMEDCT_US,SNOMEDCT_VET --conj -V USAbase\"\n",
    "\n",
    "#Retrieve MetaMap results for each trial\n",
    "for e,nct in zip(extraction, trials):\n",
    "    \n",
    "    #Pull out the terms for the trial\n",
    "    terms = []\n",
    "    for i in range(0, len(e)):\n",
    "        for text in e[i].values():\n",
    "            for t in text:\n",
    "                terms.append(t[0])\n",
    "    \n",
    "    with open('temp.txt', \"w\") as mm_input:\n",
    "        for t in terms:\n",
    "            mm_input.write(t)\n",
    "            mm_input.write(\"\\n\\n\")\n",
    "    \n",
    "    #Convert all non-ASCII characters using UMLS provided .jar file\n",
    "    subprocess.run(\"java -jar replace_utf8.jar temp.txt > input.txt\", shell = True)\n",
    "    \n",
    "    #Submit to MetaMap\n",
    "    subprocess.run(run_mm + \" input.txt ./MM_Results/\" + nct + \"_MM.txt\", shell = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Close the MetaMap Servers\n",
    "subprocess.run([mm_path + \"/bin/skrmedpostctl stop\"], shell = True)\n",
    "subprocess.run([mm_path + \"/bin/wsdserverctl stop\"], shell = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send term with domain to ConceptHub and save concept, vocab, and score\n",
    "Export the output of each to (NCTID)\\_ch.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests as r\n",
    "import json\n",
    "\n",
    "url = 'http://149.28.237.139:8002/concepthub/omop/searchOneEntityByTermAndDomain'\n",
    "for e,nct in zip(extraction, trials):\n",
    "    \n",
    "    #Send the terms and domains pairs for the trial to ConceptHub and save query and mapping\n",
    "    responses = []\n",
    "    for i in range(0, len(e)):\n",
    "        for text in e[i].values():\n",
    "            for t in text:\n",
    "                query = {'term':t[0], 'domain':t[1]}\n",
    "                reply = r.post(url, json = query)\n",
    "                responses.append([query,reply.text])\n",
    "    \n",
    "    #Store the ConceptHub mappings for the trial\n",
    "    with open('./ConceptHub_Results/'+ nct + '_ch.txt', \"w\") as ch_output:\n",
    "        for res in responses:\n",
    "            ch_output.write(str(res[0])+\"\\n\")\n",
    "            #Catch HTTP Status 500 Error when term is not found\n",
    "            if (res[1][:15] == \"<!doctype html>\"):\n",
    "                ch_output.write(\"N/A\")\n",
    "            else:\n",
    "                ch_output.write(res[1])\n",
    "            ch_output.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of dataframes for each trial\n",
    "ch_dataframes = []\n",
    "\n",
    "#Provide the eval() function a source to replace \"null\" in dictionary conversion\n",
    "null = False\n",
    "\n",
    "#Opens each results txt file and extracts each relevant information for dataframe\n",
    "ch_results = []\n",
    "for nct in trials:\n",
    "    trial = [nct]\n",
    "    buffer = []\n",
    "    with open('./ConceptHub_Results/' + nct + '_ch.txt', \"r\") as ch_file:\n",
    "        for line in ch_file:\n",
    "            if line != \"\\n\" and line != \"\\n\\n\":\n",
    "                buffer.append(line)\n",
    "                if line.startswith(\"{\\\"matchScore\") or line.startswith(\"N/A\"):\n",
    "                    trial.append(\"\".join(buffer))\n",
    "                    buffer = []    \n",
    "    ch_results.append(trial)\n",
    "\n",
    "for t in ch_results:\n",
    "    nctid = t[0]\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    #Go through each query and convert the text into dictionaries for better parsing\n",
    "    #Also converts mappings to dictionaires\n",
    "    for i in range(1, len(t)):\n",
    "        s = t[i].splitlines()\n",
    "        \n",
    "        query = eval(s[0])\n",
    "        term = query.get('term')\n",
    "        domain = query.get('domain')\n",
    "        \n",
    "        #Account for unmapped queries\n",
    "        if (s[1] != \"N/A\"):\n",
    "            match = eval(s[1])\n",
    "            score = match.get('matchScore')\n",
    "            c = match.get('concept')\n",
    "            c_ID = c.get('conceptId')\n",
    "            concept = c.get('conceptName')\n",
    "            vocab = c.get('vocabularyId')\n",
    "            c_class = c.get('conceptClassId')\n",
    "            standard = c.get('standardConcept')\n",
    "            source_ID = c.get('conceptCode')\n",
    "            \n",
    "            d = pd.DataFrame({'NCTID':[nctid], 'Term':[term], 'Domain':[domain], \n",
    "                                  'Score':[float(score)], 'Concept ID':[c_ID], 'Concept Name':[concept], \n",
    "                                  'Vocab':[vocab], 'Source Code':[source_ID], 'Standard':[standard],\n",
    "                                  'Concept Class ID':[c_class]})\n",
    "        else:\n",
    "            d = pd.DataFrame({'NCTID':[nctid], 'Term':[term], 'Domain':[domain], \n",
    "                                  'Score':[float(0)], 'Concept ID':['N/A'], 'Concept Name':['N/A'], \n",
    "                                  'Vocab':['N/A'], 'Source Code':['N/A'], 'Standard':['N/A'],\n",
    "                                  'Concept Class ID':['N/A']})\n",
    "        \n",
    "        df = df.append(d, ignore_index = True)\n",
    "    \n",
    "    ch_dataframes.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract MetaMap score, CUI, source vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the index of a substring from a list of strings\n",
    "def index_of(substring, l):\n",
    "    for x, elem in enumerate(l):\n",
    "        if substring in elem:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of dataframes for each trial\n",
    "mm_dataframes = []\n",
    "\n",
    "#Opens each results txt file and extracts each relevant information for dataframe\n",
    "mm_results = []\n",
    "for nct in trials:\n",
    "    trial = [nct]\n",
    "    with open('./MM_Results/' + nct + '_MM.txt', \"r\") as mm_file:\n",
    "        buffer = []\n",
    "        for line in mm_file:\n",
    "            if line.startswith(\"Processing\") and \"<<<<< Phrase\\n\" in buffer:\n",
    "                trial.append(\"\".join(buffer))\n",
    "                buffer = []\n",
    "            \n",
    "            buffer.append(line)\n",
    "            if line.startswith(\"<<<<< Mappings\"):\n",
    "                trial.append(\"\".join(buffer))\n",
    "                buffer = []    \n",
    "    mm_results.append(trial)\n",
    "\n",
    "for t in mm_results:\n",
    "    nctid = t[0]\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    #p indicates how far back from a multi-phrase input to look for the input text\n",
    "    p = 0\n",
    "    \n",
    "    #Checks to see if there are multiple mappings for a given entry\n",
    "    for i in range(1, len(t)):\n",
    "        \n",
    "        # Under the conditions provided to MetaMap in mm_path, all individual mappings have the same score\n",
    "        # as the overall score. So we assume that we can check for the count of that score surrounded by spaces as\n",
    "        # to not confuse it with any CUI that contains the score as a substring. Also, we need to account for a given\n",
    "        # input being broken down into multiple phrases.\n",
    "        \n",
    "        s = t[i].splitlines()\n",
    "\n",
    "        #If the input has been broken down, t[i][0] will be '', otherwise 'Processing input...'\n",
    "        if (s[0] == ''):\n",
    "            p = p + 1\n",
    "        else:\n",
    "            p = 0\n",
    "        \n",
    "        #Input - if its been broken down, pull from t[i-1]\n",
    "        if (p != 0):\n",
    "            sp = t[i-p].splitlines()\n",
    "            input_txt = sp[0][sp[0].index(\":\")+2:]\n",
    "        else: \n",
    "            input_txt = s[0][s[0].index(\":\")+2:]\n",
    "        \n",
    "        #j indicates the index of interest\n",
    "        j = index_of('Phrase: ', s)\n",
    "        \n",
    "        #Phrase - for exact matching with C2Q entity\n",
    "        phrase = s[j][s[j].index(\":\")+2:]\n",
    "\n",
    "        #Some inputs don't map, so those must be accounted for as well\n",
    "        if (\"Mappings\" not in t[i]):\n",
    "            d = pd.DataFrame({'NCTID':[nctid], 'Input':[input_txt], 'Phrase':[phrase], \n",
    "                              'Score':[0], 'CUI':[\"N/A\"], 'String':[\"N/A\"], \n",
    "                              'Preferred Name':[\"N/A\"], 'Vocab':[\"N/A\"], 'Semantic Type':[\"N/A\"]})\n",
    "            df = df.append(d)\n",
    "        else:\n",
    "            j = index_of('Meta Mapping', s)\n",
    "            score = s[j][s[j].index(\"(\")+1:s[j].index(\")\")]\n",
    "            #Formatted for count function\n",
    "            f_score = \" \" + score + \" \"\n",
    "\n",
    "            #c is used to determine which mapping to pull\n",
    "            for c in range(t[i].count(f_score)):\n",
    "                j = index_of(f_score,s) + c\n",
    "                \n",
    "                #CUI\n",
    "                cui = s[j][s[j].index(score)+6:s[j].index(\":\")].strip()\n",
    "\n",
    "                #Not all mappings include a specified UMLS preferred name, so it must be checked for\n",
    "                if \"})\" in s[j]:\n",
    "                    #UMLS Matched String\n",
    "                    if \") (\" in s[j]:\n",
    "                        ms = s[j][s[j].index(\":\")+1:s[j].index(\") \")]\n",
    "                    else:\n",
    "                        ms = s[j][s[j].index(\":\")+1:s[j].index(\" (\")]\n",
    "                    #UMLS Preferred Name\n",
    "                    name = s[j][s[j].index(\"(\")+1:s[j].index(\" {\")]\n",
    "                #If it is not present, the matched string will be used as the preferred name\n",
    "                else:\n",
    "                    if \"(\" in s[j]:\n",
    "                        ms = s[j][s[j].index(\":\")+1:s[j].index(\")\")+1]\n",
    "                    else:\n",
    "                        ms = s[j][s[j].index(\":\")+1:s[j].index(\"{\")]\n",
    "                    name = ms\n",
    "\n",
    "                #Source Vocabularies\n",
    "                vocab = s[j][s[j].index(\"{\")+1:s[j].index(\"}\")]\n",
    "\n",
    "                #Semantic Type\n",
    "                s_type = s[j][s[j].index(\"[\")+1:s[j].index(\"]\")]\n",
    "\n",
    "                #Add query to the dataframe\n",
    "                d = pd.DataFrame({'NCTID':[nctid], 'Input':[input_txt], 'Phrase':[phrase], \n",
    "                                  'Score':[int(score)], 'CUI':[cui], 'String':[ms], \n",
    "                                  'Preferred Name':[name], 'Vocab':[vocab], 'Semantic Type':[s_type]})\n",
    "                df = df.append(d, ignore_index = True)\n",
    "                \n",
    "    #Add the dataframe for this trial to the list of dataframes\n",
    "    mm_dataframes.append(df)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_dataframes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_dataframes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send CUIs to UMLS API to Convert Into AUI\n",
    "Optional component to be worked on later\n",
    "For My_API_Key, follow the steps at: https://documentation.uts.nlm.nih.gov/rest/authentication.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MetaMap Results\n",
    "for df in mm_dataframes:\n",
    "    df.to_csv(r''+df.iloc[0]['NCTID']+'MM_table.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Authentication.py from Steven P. Emrick with UMLS\n",
    "#from pyquery import PyQuery as pq\n",
    "import lxml.html as lh\n",
    "from lxml.html import fromstring\n",
    "\n",
    "uri=\"https://utslogin.nlm.nih.gov\"\n",
    "#option 1 - username/pw authentication at /cas/v1/tickets\n",
    "#auth_endpoint = \"/cas/v1/tickets/\"\n",
    "#option 2 - api key authentication at /cas/v1/api-key\n",
    "auth_endpoint = \"/cas/v1/api-key\"\n",
    "\n",
    "class Authentication:\n",
    "\n",
    "    #def __init__(self, username,password):\n",
    "    def __init__(self, apikey):\n",
    "        #self.username=username\n",
    "        #self.password=password\n",
    "        self.apikey=apikey\n",
    "        self.service=\"http://umlsks.nlm.nih.gov\"\n",
    "    \n",
    "    def gettgt(self):\n",
    "        #params = {'username': self.username,'password': self.password}\n",
    "        params = {'apikey': self.apikey}\n",
    "        h = {\"Content-type\": \"application/x-www-form-urlencoded\", \"Accept\": \"text/plain\", \"User-Agent\":\"python\" }\n",
    "        rep = r.post(uri+auth_endpoint,data=params,headers=h)\n",
    "        response = fromstring(rep.text)\n",
    "        ## extract the entire URL needed from the HTML form (action attribute) returned - looks similar to https://utslogin.nlm.nih.gov/cas/v1/tickets/TGT-36471-aYqNLN2rFIJPXKzxwdTNC5ZT7z3B3cTAKfSc5ndHQcUxeaDOLN-cas\n",
    "        ## we make a POST call to this URL in the getst method\n",
    "        tgt = response.xpath('//form/@action')[0]\n",
    "        return tgt\n",
    "\n",
    "    def getst(self,tgt):\n",
    "        params = {'service': self.service}\n",
    "        h = {\"Content-type\": \"application/x-www-form-urlencoded\", \"Accept\": \"text/plain\", \"User-Agent\":\"python\" }\n",
    "        rep = r.post(tgt,data=params,headers=h)\n",
    "        st = rep.text\n",
    "        return st\n",
    "\n",
    "#Need to use the API key to retrieve Ticket Granting Ticket and \n",
    "#a unique one-time use Service Ticket for each call to the API\n",
    "\n",
    "My_API_Key = \"replace-with-your-key\"\n",
    "auth = Authentication(My_API_Key)\n",
    "tgt = auth.gettgt()\n",
    "uri = \"https://uts-ws.nlm.nih.gov/rest/\"\n",
    "\n",
    "#Go through each trial and take out the preferred term\n",
    "for m in mm_dataframes:\n",
    "    for i in range(len(m)):\n",
    "        st = auth.getst(tgt)\n",
    "        content_endpoint = \"/content/current/CUI/\"+m.loc[i,'CUI']+\"/atoms\"\n",
    "        query = {'sabs':'CVX, HCPCS, ICD10PCS, LNC, RXNORM, SNOMEDCT_US, SNOMEDCT_VET',\n",
    "                 'ttys':'PT, LN, CN, LA, PSN','ticket':st}\n",
    "        \n",
    "        p = r.get(uri+content_endpoint, params = query)\n",
    "        parsed = eval(p.text)\n",
    "        code = parsed.get('result')[0].get('code')\n",
    "        s_code = code[code.rfind('/')+1:]\n",
    "        s_vocab = parsed.get('result')[0].get('rootSource')\n",
    "        print (m.loc[i,'Phrase'],s_code, s_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from owlready2 import *\n",
    "from owlready2.pymedtermino2 import *\n",
    "from owlready2.pymedtermino2.umls import *\n",
    "\n",
    "import_umls(\"umls-2019AB-metathesaurus.zip\")\n",
    "default_world.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse dataframes to extract best concepts\n",
    "Creates three different dataframes\n",
    "1. MetaMap results supplemented by ConceptHub\n",
    "2. ConceptHub results supplemented by MetaMap\n",
    "3. Highest score supplemented by MetaMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "c2qp_1 = []\n",
    "c2qp_2 = []\n",
    "c2qp_3 = []\n",
    "\n",
    "for m, u in zip(mm_dataframes, ch_dataframes):\n",
    "    #Dataframe for each of the methods\n",
    "    df1 = pd.DataFrame()\n",
    "    df2 = pd.DataFrame()\n",
    "    df3 = pd.DataFrame()\n",
    "    \n",
    "    #Relies on ConceptHub for the terms (or 'input', in MetaMap)\n",
    "    terms = u.T.loc['Term']\n",
    "    for i in range(len(terms)):\n",
    "        t = terms[i]\n",
    "        #Convert º to degrees and adjust any regex characters\n",
    "        if \"°\" in t:\n",
    "            t = t[:t.index(\"°\")] + \"degrees \" + t[t.index(\"°\")+1:]\n",
    "        \n",
    "        #Escapes any regex characters before using str.contains\n",
    "        t = re.escape(t)\n",
    "        \n",
    "        #Rows of interest for parsing\n",
    "        m_temp = m[m['Input'].str.contains(t)]\n",
    "        u_temp = u.iloc[i]\n",
    "        \n",
    "        #Method 1\n",
    "        for row in m_temp.iterrows():\n",
    "            nctid = row[1]['NCTID']\n",
    "            text = row[1]['Input']\n",
    "            if row[1]['CUI'] != \"N/A\":\n",
    "                domain = row[1]['Semantic Type']\n",
    "                concept_id = row[1]['CUI']\n",
    "                concept = row[1]['String']\n",
    "                score = row[1]['Score']\n",
    "                \n",
    "                d = pd.DataFrame({'NCTID':[nctid], 'Text':[text], 'Domain':[domain], 'Mapping':['M'], 'Concept ID':[concept_id],\n",
    "                                 'Concept':[concept],'Score':[score]})\n",
    "                df1 = df1.append(d,ignore_index = True)\n",
    "            elif u_temp['Concept ID'] != \"N/A\":\n",
    "                domain = u_temp['Domain']\n",
    "                concept_id = u_temp['Concept ID']\n",
    "                concept = u_temp['Concept Name']\n",
    "                #To match MetaMap scoring format\n",
    "                score = int(u_temp['Score']*1000)\n",
    "                \n",
    "                d = pd.DataFrame({'NCTID':[nctid], 'Text':[text], 'Domain':[domain], 'Mapping':['C'], 'Concept ID':[concept_id],\n",
    "                                 'Concept':[concept],'Score':[score]})\n",
    "                df1 = df1.append(d,ignore_index = True)\n",
    "            else:\n",
    "                d = pd.DataFrame({'NCTID':[nctid], 'Text':[text], 'Domain':[domain],'Mapping':['N'], 'Concept ID':[\"N/A\"],\n",
    "                                 'Concept':[\"N/A\"],'Score':[0]})\n",
    "                df1 = df1.append(d,ignore_index = True)\n",
    "        \n",
    "        #Method 2\n",
    "        for row in m_temp.iterrows():\n",
    "            nctid = row[1]['NCTID']\n",
    "            text = row[1]['Input']\n",
    "            if u_temp['Concept ID'] != \"N/A\":\n",
    "                domain = u_temp['Domain']\n",
    "                concept_id = u_temp['Concept ID']\n",
    "                concept = u_temp['Concept Name']\n",
    "                #To match MetaMap scoring format\n",
    "                score = int(u_temp['Score']*1000)\n",
    "                \n",
    "                d = pd.DataFrame({'NCTID':[nctid], 'Text':[text], 'Domain':[domain], 'Mapping':['C'], 'Concept ID':[concept_id],\n",
    "                                 'Concept':[concept],'Score':[score]})\n",
    "                df2 = df2.append(d,ignore_index = True)\n",
    "            elif row[1]['CUI'] != \"N/A\":\n",
    "                domain = row[1]['Semantic Type']\n",
    "                concept_id = row[1]['CUI']\n",
    "                concept = row[1]['String']\n",
    "                score = row[1]['Score']\n",
    "                \n",
    "                d = pd.DataFrame({'NCTID':[nctid], 'Text':[text], 'Domain':[domain], 'Mapping':['M'], 'Concept ID':[concept_id],\n",
    "                                 'Concept':[concept],'Score':[score]})\n",
    "                df2 = df2.append(d,ignore_index = True)\n",
    "            else:\n",
    "                d = pd.DataFrame({'NCTID':[nctid], 'Text':[text], 'Domain':[domain],'Mapping':['N'], 'Concept ID':[\"N/A\"],\n",
    "                                 'Concept':[\"N/A\"],'Score':[0]})\n",
    "                df2 = df2.append(d,ignore_index = True)\n",
    "        \n",
    "        #Method 3\n",
    "        for row in m_temp.iterrows():\n",
    "            nctid = row[1]['NCTID']\n",
    "            text = row[1]['Input']\n",
    "            if u_temp['Concept ID'] != \"N/A\" and row[1]['CUI'] != \"N/A\":\n",
    "                u_score = int(u_temp['Score']*1000)\n",
    "                m_score = row[1]['Score']\n",
    "                \n",
    "                #Assumes that MetaMapping is better if scores are equal\n",
    "                if u_score > m_score:\n",
    "                    domain = u_temp['Domain']\n",
    "                    concept_id = u_temp['Concept ID']\n",
    "                    concept = u_temp['Concept Name']\n",
    "                    score = u_score\n",
    "\n",
    "                    d = pd.DataFrame({'NCTID':[nctid], 'Text':[text], 'Domain':[domain], 'Mapping':['C'], 'Concept ID':[concept_id],\n",
    "                                     'Concept':[concept],'Score':[score]})\n",
    "                    df3 = df3.append(d,ignore_index = True)\n",
    "                else:\n",
    "                    domain = row[1]['Semantic Type']\n",
    "                    concept_id = row[1]['CUI']\n",
    "                    concept = row[1]['String']\n",
    "                    score = m_score\n",
    "\n",
    "                    d = pd.DataFrame({'NCTID':[nctid], 'Text':[text], 'Domain':[domain], 'Mapping':['M'], 'Concept ID':[concept_id],\n",
    "                                     'Concept':[concept],'Score':[score]})\n",
    "                    df3 = df3.append(d,ignore_index = True)\n",
    "\n",
    "    c2qp_1.append(df1)\n",
    "    c2qp_2.append(df2)\n",
    "    c2qp_3.append(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizes the concept reviewal process\n",
    "for d1, d2, d3 in zip(c2qp_1, c2qp_2, c2qp_3):\n",
    "    d1.drop_duplicates(inplace = True)\n",
    "    d2.drop_duplicates(inplace = True)\n",
    "    d3.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Excel file from each dataframe\n",
    "Format: C2QP_[Method].xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Sheet\n",
    "with pd.ExcelWriter('C2QP_MC.xlsx') as writer:\n",
    "    for df in c2qp_1:\n",
    "        df.to_excel(writer, sheet_name = df.iloc[0]['NCTID'], index = False)\n",
    "#Second Sheet\n",
    "with pd.ExcelWriter('C2QP_CM.xlsx') as writer:\n",
    "    for df in c2qp_2:\n",
    "        df.to_excel(writer, sheet_name = df.iloc[0]['NCTID'], index = False)\n",
    "#Third Sheet\n",
    "with pd.ExcelWriter('C2QP_BS.xlsx') as writer:\n",
    "    for df in c2qp_3:\n",
    "        df.to_excel(writer, sheet_name = df.iloc[0]['NCTID'], index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
